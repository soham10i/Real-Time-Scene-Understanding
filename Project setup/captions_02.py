import os
import cv2
import torch
from PIL import Image
from ultralytics import YOLO  # YOLOv8 library
from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline


def setup_yolov8():
    """
    Sets up the YOLOv8 model for segmentation and detection, with GPU support.
    Returns:
        model (YOLO): Pre-trained YOLOv8 model.
    """
    print("Loading YOLOv8 model...")
    model = YOLO('yolov8n-seg.pt')  # YOLOv8 segmentation model
    if torch.cuda.is_available():
        model.to('cuda')  # Move the model to GPU
        print("YOLOv8 model loaded successfully and moved to GPU.")
    else:
        raise EnvironmentError("GPU is not available. Please ensure CUDA is properly installed.")
    return model


def setup_blip():
    """
    Sets up the BLIP model and processor, with GPU support.
    Returns:
        processor (BlipProcessor): Preprocessing pipeline for images.
        model (BlipForConditionalGeneration): BLIP model for caption generation.
    """
    print("Loading BLIP model and processor...")
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    if torch.cuda.is_available():
        model.to('cuda')  # Move the model to GPU
        print("BLIP model loaded successfully and moved to GPU.")
    else:
        raise EnvironmentError("GPU is not available. Please ensure CUDA is properly installed.")
    return processor, model


def setup_summarizer():
    """
    Sets up the summarizer pipeline using a pre-trained model.
    Returns:
        summarizer: Hugging Face summarization pipeline.
    """
    print("Loading summarization model...")
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=0)  # Use GPU (device=0)
    print("Summarization model loaded successfully.")
    return summarizer


def generate_caption(processor, model, image):
    """
    Generates a caption for a given image using the BLIP model, with GPU support.

    Args:
        processor (BlipProcessor): BLIP processor for input preparation.
        model (BlipForConditionalGeneration): BLIP model for caption generation.
        image (PIL.Image): Image for which the caption is to be generated.

    Returns:
        str: Generated caption.
    """
    inputs = processor(images=image, return_tensors="pt").to('cuda')  # Move inputs to GPU
    output = model.generate(**inputs, max_new_tokens=128)
    caption = processor.decode(output[0], skip_special_tokens=True)
    return caption


def generate_scene_summary(captions, summarizer):
    """
    Generates a summary of the scene using an LLM and all BLIP-generated captions.

    Args:
        captions (List[str]): List of captions generated by BLIP for the frame.
        summarizer: Hugging Face summarization pipeline.

    Returns:
        str: Summary of the scene.
    """
    input_text = " ".join(captions)  # Combine all captions into one text
    if len(input_text) > 1024:  # Limit input to model's token length
        input_text = input_text[:1024]

    # Generate summary using the pipeline (runs on GPU)
    summary = summarizer(input_text, max_length=50, min_length=10, do_sample=False)
    return summary[0]["summary_text"]


def process_yolo_output(results, frame):
    """
    Processes YOLOv8 results to extract and annotate masked images.

    Args:
        results (Results): YOLOv8 output results.
        frame (numpy.ndarray): Original frame.

    Returns:
        List[PIL.Image]: List of masked images as PIL Image objects.
    """
    result = results[0]
    masked_images = []
    frame_height, frame_width = frame.shape[:2]

    for box, mask in zip(result.boxes.xyxy, result.masks.data):
        x1, y1, x2, y2 = map(int, box.tolist())
        # Ensure coordinates are within frame bounds
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(frame_width, x2), min(frame_height, y2)

        # Convert mask to binary and resize to fit ROI
        mask = (mask.cpu().numpy() * 255).astype("uint8")
        mask_resized = cv2.resize(mask, (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)

        # Extract the region of interest (ROI)
        roi = frame[y1:y2, x1:x2]

        # Apply the mask to the ROI
        if roi.size > 0 and mask_resized.size > 0:  # Ensure ROI and mask are non-empty
            masked_roi = cv2.bitwise_and(roi, roi, mask=mask_resized)
            masked_images.append(Image.fromarray(cv2.cvtColor(masked_roi, cv2.COLOR_BGR2RGB)))

    return masked_images



def process_frame_with_summary(frame, results, processor, blip_model):
    """
    Processes a frame to generate captions for all detected objects.

    Args:
        frame (numpy.ndarray): Original frame.
        results (Results): YOLOv8 results.
        processor (BlipProcessor): BLIP processor for input preparation.
        blip_model (BlipForConditionalGeneration): BLIP model for caption generation.

    Returns:
        List[str]: Captions generated for all detected objects in the frame.
    """
    masked_images = process_yolo_output(results, frame)
    captions = [generate_caption(processor, blip_model, img) for img in masked_images]
    return captions


def main():
    yolo_model = setup_yolov8()
    processor, blip_model = setup_blip()
    summarizer = setup_summarizer()

    input_video_path = "classroom_video.mp4"  # Update this with the actual video file path
    output_video_path = "processed_video.mp4"  # Output file name

    cap = cv2.VideoCapture(input_video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video file {input_video_path}.")
        return

    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

    print("Processing video... This may take some time.")
    frame_count = 0
    all_captions = []  # Store captions for all frames

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            print("End of video or error reading the frame.")
            break

        # Perform segmentation and detection using YOLOv8
        results = yolo_model(frame, conf=0.5, iou=0.4)

        # Generate captions for the current frame
        frame_captions = process_frame_with_summary(frame, results, processor, blip_model)
        all_captions.extend(frame_captions)  # Collect captions across all frames

        # Annotate the frame with frame number
        text_box_height = 50  # Height of the text box
        frame_with_box = cv2.rectangle(frame, (0, 0), (frame.shape[1], text_box_height), (0, 0, 0), -1)  # Black box
        cv2.putText(frame_with_box, f"Frame {frame_count + 1}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

        # Write the processed frame to the output video
        out.write(frame_with_box)

        frame_count += 1
        print(f"Processed frame {frame_count}/{total_frames}")

    cap.release()
    out.release()

    # Generate a single summary for the entire video
    print("Generating final summary...")
    final_summary = generate_scene_summary(all_captions, summarizer)
    print("Final Summary of Video:")
    print(final_summary)


if __name__ == "__main__":
    main()
